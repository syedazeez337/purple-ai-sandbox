# AI Development Policy with Comprehensive Monitoring
# This policy demonstrates all AI-specific features including:
# - LLM API monitoring and interception
# - Token and cost budget enforcement
# - Rate limiting for API calls
# - Security scanning for prompts
# - Detailed usage tracking and logging

name: "ai-development-safe"
description: "Secure AI development environment with comprehensive LLM monitoring and budget enforcement"

# Filesystem configuration
filesystem:
  immutable_paths:
    - host_path: "/usr/bin"
      sandbox_path: "/usr/bin"
    - host_path: "/usr/lib"
      sandbox_path: "/usr/lib"
    - host_path: "/usr/lib64"
      sandbox_path: "/usr/lib64"
    - host_path: "/lib"
      sandbox_path: "/lib"
    - host_path: "/lib64"
      sandbox_path: "/lib64"
    - host_path: "/bin"
      sandbox_path: "/bin"
  scratch_paths:
    - "/tmp"
  output_paths:
    - host_path: "./output/ai-results"
      sandbox_path: "/output"
  working_dir: "/tmp"

# Syscall filtering
syscalls:
  default_deny: false
  allow: []
  deny:
    - "mount"
    - "umount2"
    - "reboot"
    - "kexec_load"
    - "bpf"
    - "ptrace"

# Resource limits
resources:
  cpu_shares: 1.0
  memory_limit_bytes: "2G"
  pids_limit: 50
  block_io_limit: "50MBps"
  session_timeout_seconds: 3600

# Capability management
capabilities:
  default_drop: true
  add: []

# Network configuration with AI monitoring
network:
  isolated: true
  allow_outgoing:
    - "80"
    - "443"
  allow_incoming: []

# Audit configuration
audit:
  enabled: true
  log_path: "/var/log/purple/ai-development.log"
  detail_level:
    - "api_calls"
    - "budget_usage"
    - "security_events"

# AI-Specific Policies (NEW!)
ai_policy:
  # Budget limits for token and cost control
  budget:
    max_tokens: 100000  # 100K tokens limit
    max_cost: "$50.00"  # $50.00 cost limit

  # Rate limiting to prevent abuse
  rate_limits:
    requests_per_minute: 60  # 60 requests per minute
    tokens_per_minute: 50000 # 50K tokens per minute

  # Allowed LLM API providers and models
  llm_apis:
    block_unknown: true  # Block unknown providers
    providers:
      - name: "openai"
        endpoints:
          - "https://api.openai.com/v1/chat/completions"
          - "https://api.openai.com/v1/embeddings"
        models:
          - "gpt-4"
          - "gpt-4-turbo"
          - "gpt-3.5-turbo"
      - name: "anthropic"
        endpoints:
          - "https://api.anthropic.com/v1/messages"
        models:
          - "claude-3-5-sonnet-20241022"
          - "claude-3-opus-20240229"

  # Security settings for prompt analysis
  security:
    prompt_injection_detection: true
    sensitive_data_scanning: true

  # Monitoring and logging settings
  monitoring:
    log_prompts: false  # Don't log actual prompts for privacy
    log_responses: false # Don't log actual responses for privacy
    log_tokens: true    # Log token usage statistics
    log_costs: true     # Log cost information

# Example usage:
# purple run --profile ai-development-safe --command "python3 ai_agent.py"
# This will:
# 1. Create an isolated sandbox with network access
# 2. Monitor all LLM API calls through the HTTP proxy
# 3. Enforce token and cost budgets
# 4. Apply rate limiting
# 5. Scan for security issues
# 6. Provide comprehensive usage reporting