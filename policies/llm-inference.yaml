# Policy 6: LLM Inference Server
# Use Case: Serving LLM models via API (vLLM, TGI, Ollama)
# Security Level: Medium - high compute, network for API serving
#
# Best Practices Applied:
# - Uses default_deny: false with explicit deny list
# - Output paths use /tmp/purple/
# - Working directory is /tmp

name: "llm-inference"
description: "Sandbox for LLM inference servers. Allows high memory and incoming network for API serving."

filesystem:
  immutable_paths:
    - host_path: "/usr"
      sandbox_path: "/usr"
    - host_path: "/lib"
      sandbox_path: "/lib"
    - host_path: "/lib64"
      sandbox_path: "/lib64"
    - host_path: "/bin"
      sandbox_path: "/bin"
    - host_path: "/etc/ssl"
      sandbox_path: "/etc/ssl"
    - host_path: "/etc/pki"
      sandbox_path: "/etc/pki"
  scratch_paths:
    - "/tmp"
    - "/var/tmp"
  output_paths:
    - host_path: "/tmp/purple/models"
      sandbox_path: "/models"
    - host_path: "/tmp/purple/output/inference-logs"
      sandbox_path: "/logs"
  working_dir: "/tmp"

syscalls:
  default_deny: false
  allow: []
  deny:
    # Kernel/system modification
    - "mount"
    - "umount2"
    - "reboot"
    - "kexec_load"
    - "kexec_file_load"
    - "init_module"
    - "finit_module"
    - "delete_module"
    - "swapon"
    - "swapoff"
    - "sethostname"
    - "setdomainname"
    - "pivot_root"
    - "chroot"
    # Process debugging/tracing
    - "ptrace"
    - "process_vm_readv"
    - "process_vm_writev"
    # Privilege escalation
    - "setuid"
    - "setgid"
    - "setreuid"
    - "setregid"
    - "setresuid"
    - "setresgid"
    - "capset"
    # Security bypass
    - "personality"
    - "seccomp"
    - "bpf"
    - "perf_event_open"
    - "userfaultfd"
    - "io_uring_setup"
    - "io_uring_enter"
    - "io_uring_register"

resources:
  cpu_shares: 0.9
  memory_limit_bytes: "64G"
  pids_limit: 100
  block_io_limit: "500MBps"
  session_timeout_seconds: 86400

capabilities:
  default_drop: true
  add: []

network:
  isolated: false
  allow_outgoing:
    - "443"
    - "80"
  allow_incoming:
    - "8080"  # API port

audit:
  enabled: true
  log_path: "/var/log/purple/llm-inference.log"
  detail_level:
    - "network"
    - "resource"
